---
title: "BA810 Team 7: Acovado Price Modeling"
author: Ziqin Ma, Qiaoling Huang, Shihan Li, Chenran Peng, Elmira Ushirova
date: October 16, 2019
output: html_notebook
---

----------

## Introduction ##

* ### Business Setting ###

(who cares and why)

* ### Business Problem ###

--------------

## Dataset Debrief ##

* ### Data Source ###

* ### Descriptive Analysis ###

----------

## Data Preparation ##

Before looking at the dataset, we first load the packages that will be used.
```{r}
options(stringsAsFactors = FALSE)

library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggthemes)
library(fastDummies)
library(scales)
library(glmnet)
library(zoo)
library(rpart)
library(rpart.plot)
library(tree)
library(randomForest)
library(gbm)

# Set the ggtheme beforehead for all plots
theme_set(theme_clean())

```

Then we load the original kaggle dataset and print a quick view of columns and data types.
```{r}
ds <- read_csv("avocado.csv")  
glimpse(ds)
```
There are 14 columns in this dataset, and we will use `AveragePrice` as our prediction. `X1` gives a id for each observation, and we want to transfer that into row numbers for each row by adding 1. Most of the data types are `<dbl>`, and there are only 2 columns with character vectors, `type` and `region`. We will transfer them to dummy variables to train our models.

In the following code chunk, we took multiple steps to arrange the columns.
```{r}
# Switch the order of rows
ds <- ds %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)

# Change `X1` to `ID`
colName <- names(ds)
colName[1] <- "ID"
names(ds) <- colName

# Assign distinct number id to each observation in `ID` column
ds$ID <- seq(nrow(ds))

# Extract month from `Date` column
ds$month <- month(ds$Date)
ds <- ds %>% 
  select(ID, year, month, everything())

```


There are only 2 types of avocados recorded, conventional and organic. We decided to separate `type` column into `type_conventional` and `type_organic`.
```{r}
dsNew <- dummy_cols(ds, select_columns = "type") %>% 
  select(ID, year, month, region, type_conventional, type_organic, 
         everything(), -type)

```

The Product Look Up (PLU) codes are used by grocery retailers to make differnet product inventories. This dataset includes 3 PLU avocados, and we added a new column `other_PLU` to see the volume sold in products without PLU. The number is calculated by substracting PLU volumes from total volumes.
```{r}
dsNew$other_PLU <- dsNew$`Total Volume` - dsNew$`4046` - dsNew$`4225` - dsNew$`4770`

# Reorder the columes
dsNew <- dsNew %>% 
  select(1:3, Date, everything())
```
We initially included `other_PLU` as a predictor to our models, but then we found that `other_PLU` is eauql to `Total Bags`. This equivlance resolved our confusion on bag variables: ?????


Categorize `region` into US Areas
```{r}
uniqueRegion <- unique(dsNew$region)
uniqueRegion <- as.data.frame(uniqueRegion)
uniqueRegion$Area <- NA
uniqueRegion$Area[1] <- "NewEngland"
uniqueRegion$Area[2] <- "Southeast"
uniqueRegion$Area[3] <- "Mideast"
uniqueRegion$Area[4] <- "RockyMountain"
uniqueRegion$Area[5] <- "NewEngland"
uniqueRegion$Area[6] <- "Mideast"
uniqueRegion$Area[7] <- "FarWest"
uniqueRegion$Area[8] <- "Southeast"
uniqueRegion$Area[9] <- "GreatLakes"
uniqueRegion$Area[10] <- "GreatLakes"
uniqueRegion$Area[11] <- "GreatLakes"
uniqueRegion$Area[12] <- "Southwest"
uniqueRegion$Area[13] <- "RockyMountain"
uniqueRegion$Area[14] <- "GreatLakes"
uniqueRegion$Area[15] <- "GreatLakes"
uniqueRegion$Area[16] <- "GreatLakes"
uniqueRegion$Area[17] <- "Mideast"
uniqueRegion$Area[18] <- "NewEngland"
uniqueRegion$Area[19] <- "Southeast"
uniqueRegion$Area[20] <- "GreatLakes"
uniqueRegion$Area[21] <- "Southeast"
uniqueRegion$Area[22] <- "FarWest"
uniqueRegion$Area[23] <- "FarWest"
uniqueRegion$Area[24] <- "Southeast"
uniqueRegion$Area[25] <- "Southeast"
uniqueRegion$Area[26] <- "Southeast"
uniqueRegion$Area[27] <- "Southeast"
uniqueRegion$Area[28] <- "Southeast"
uniqueRegion$Area[29] <- "Mideast"
uniqueRegion$Area[30] <- "NewEngland"
uniqueRegion$Area[31] <- "NewEngland"
uniqueRegion$Area[32] <- "Southeast"
uniqueRegion$Area[33] <- "Mideast"
uniqueRegion$Area[34] <- "Southwest"
uniqueRegion$Area[35] <- "Mideast"
uniqueRegion$Area[36] <- "Plains"
uniqueRegion$Area[37] <- "FarWest"
uniqueRegion$Area[38] <- "Southeast"
uniqueRegion$Area[39] <- "Southeast"
uniqueRegion$Area[40] <- "Southeast"
uniqueRegion$Area[41] <- "FarWest"
uniqueRegion$Area[42] <- "FarWest"
uniqueRegion$Area[43] <- "FarWest"
uniqueRegion$Area[44] <- "FarWest"
uniqueRegion$Area[45] <- "Southeast"
uniqueRegion$Area[46] <- "Southeast"
uniqueRegion$Area[47] <- "Southeast"
uniqueRegion$Area[48] <- "FarWest"
uniqueRegion$Area[49] <- "Plains"
uniqueRegion$Area[50] <- "Mideast"
uniqueRegion$Area[51] <- "Southeast"
uniqueRegion$Area[52] <- "TotalUS"
uniqueRegion$Area[53] <- "FarWest"
uniqueRegion$Area[54] <- "Southwest"
names(uniqueRegion)[1] <- "region"

avo <- dsNew %>% 
  left_join(uniqueRegion, by = "region") %>% 
  select(1:5, Area, everything())


avo <- dummy_cols(avo, select_columns = "Area")

```


Rename Column Names
```{r}
colnames(avo)

names(avo)[10] <- "TotalVolume"
names(avo)[14] <- "TotalBags"
names(avo)[15] <- "SmallBags"
names(avo)[16] <- "LargeBags"
names(avo)[17] <- "XLargeBags"
names(avo)[11] <- "PLU4046"
names(avo)[12] <- "PLU4225"
names(avo)[13] <- "PLU4770"
```

----------

## Create Train and Test Datasets ##

```{r}
set.seed(1234)
avo_train <- avo %>% filter(as.Date(Date) < "2017-03-01")
avo_train %>%
  filter(year == 2017, month == 2)
avo_test <- avo %>% filter(as.Date(Date) >= "2017-03-01")
avo_test %>%
  filter(year == 2018, month == 3)


```


Base model
```{r}
f1 <- as.formula(AveragePrice ~ month+type_conventional + type_organic + TotalVolume + 
                   PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                   + Area_NewEngland + Area_Southeast
                 + Area_Mideast + Area_RockyMountain
                 + Area_FarWest + Area_GreatLakes
                 + Area_Southwest + Area_Plains + Area_TotalUS)

x1_train <- model.matrix(f1,avo_train)[,-1]
y1_train <- avo_train$AveragePrice
x1_test <- model.matrix(f1,avo_test)[,-1]
y1_test <- avo_test$AveragePrice
x1_avo <- model.matrix(f1, avo)[,-1]
```

----------

## Modeling ##

* ### Linear Regression ###

```{r}
fit.lm <- lm(f1, avo_train)
fit.lm

y.train <- avo_train$AveragePrice
y.test <- avo_test$AveragePrice
yhat.train.lm <- predict(fit.lm)
mse.train.lm <- mean((y.train - yhat.train.lm)^2)
yhat.test.lm <- predict(fit.lm, avo_test)
mse.test.lm <- mean((y.test - yhat.test.lm)^2)
mse.train.lm
mse.test.lm

coef(fit.lm)
plot(fit.lm)
```

* ### Forward Selection ###

```{r}
avo1 <- avo
avo1$ID <- NULL
avo1$year<-NULL
avo1$Date<-NULL
avo1$region<-NULL
avo1$Area<-NULL
avo1$AveragePrice<-NULL
xnames <- colnames(avo1)
xnames <- xnames[!xnames %in% c("type_conventional","type_organic", "TotalVolume", "PLU4046", "PLU4770", "PLU4225","SmallBags", "LargeBags","XLargeBags","Area_NewEngland","Area_Southeast","Area_Mideast","Area_RockyMountain","Area_FarWest","Area_GreatLakes","Area_Southwest","Area_Plains + Area_TotalUS")]
fit_fw <- lm(AveragePrice ~ 1, data = avo_train)
yhat_train <- predict(fit_fw, avo_train)
mse_train <- mean((avo_train$AveragePrice - yhat_train) ^ 2)
yhat_test <- predict(fit_fw, avo_test)
mse_test <- mean((avo_test$AveragePrice - yhat_test) ^ 2)

log_fw <-
tibble(
xname = "intercept",
model = deparse(fit_fw$call),
mse_train = mse_train,
mse_test = mse_test
)

best_mse_train <- NA
best_mse_test <- NA
best_fit_fw <- NA
best_xname <- NA
for (xname in xnames) {
# take a moment to examine and understand the following line
fit_fw_tmp <- update(fit_fw, as.formula(paste0(" ~ ", xname)))
# compute MSE train
yhat_train_tmp <- predict(fit_fw_tmp, avo_train)
mse_train_tmp <- mean((avo_train$AveragePrice - yhat_train_tmp) ^ 2)
# compute MSE test
yhat_test_tmp <- predict(fit_fw_tmp, avo_test)
mse_test_tmp <- mean((avo_test$AveragePrice - yhat_test_tmp) ^ 2)
# if this is the first predictor to be examined,
# or if this predictors yields a lower MSE that the current
# best, then store this predictor as the current best predictor
if (is.na(best_mse_test) | mse_test_tmp < best_mse_test) {
best_xname <- xname
best_fit_fw <- fit_fw_tmp
best_mse_train <- mse_train_tmp
best_mse_test <- mse_test_tmp
}
}
best_mse_train
best_mse_test
```

```{r}
log_fw <-
log_fw %>% add_row(
xname = best_xname,
model = paste0(deparse(best_fit_fw$call), collapse = ""),
mse_train = best_mse_train,
mse_test = best_mse_test
)

log_fw

# here is a complete solution
xnames<- colnames(avo1)
xnames <- xnames[!xnames %in% c("type_conventional","type_organic", "TotalVolume", "PLU4046", "PLU4770", "PLU4225","SmallBags", "LargeBags","XLargeBags","Area_NewEngland","Area_Southeast","Area_Mideast","Area_RockyMountain","Area_FarWest","Area_GreatLakes","Area_Southwest","Area_Plains + Area_TotalUS")]
fit_fw <- lm(AveragePrice ~ 1, data = avo_train)
yhat_train <- predict(fit_fw, avo_train)
yhat_test <- predict(fit_fw, avo_test)
mse_train <- mean((avo_train$AveragePrice - yhat_train)^2)
mse_test <- mean((avo_test$AveragePrice - yhat_test)^2)
xname <- "intercept"

log_fw <-
tibble(
xname = xname,
model = paste0(deparse(fit_fw$call), collapse = ""),
mse_train = mse_train,
mse_test = mse_test
)
while (length(xnames) > 0) {
best_mse_train <- NA
best_mse_test <- NA
best_fit_fw <- NA
best_xname <- NA
# select the next best predictor
for (xname in xnames) {
# take a moment to examine and understand the following line
fit_fw_tmp <- update(fit_fw, as.formula(paste0(". ~ . + ", xname)))
# compute MSE train
yhat_train_tmp <- predict(fit_fw_tmp, avo_train)
mse_train_tmp <- mean((avo_train$AveragePrice - yhat_train_tmp) ^ 2)
# compute MSE test
yhat_test_tmp <- predict(fit_fw_tmp, avo_test)
mse_test_tmp <- mean((avo_test$AveragePrice - yhat_test_tmp) ^ 2)

# if this is the first predictor to be examined,
# or if this predictors yields a lower MSE that the current
# best, then store this predictor as the current best predictor
if (is.na(best_mse_test) | mse_test_tmp < best_mse_test) {
best_xname <- xname
best_fit_fw <- fit_fw_tmp
best_mse_train <- mse_train_tmp
best_mse_test <- mse_test_tmp
}
}
log_fw <-
log_fw %>% add_row(
xname = best_xname,
model = paste0(deparse(best_fit_fw$call), collapse = ""),
mse_train = best_mse_train,
mse_test = best_mse_test
)
# adopt the best model for the next iteration
fit_fw <- best_fit_fw
# remove the current best predictor from the list of predictors
xnames <- xnames[xnames!=best_xname]
}
```

```{r}
ggplot(log_fw, aes(seq_along(xname), mse_test)) +
geom_point() +
geom_line() +
geom_point(aes(y=mse_train), color="blue") +
geom_line(aes(y=mse_train), color="blue") +
scale_x_continuous("Variables", labels = log_fw$xname, breaks = seq_along(log_fw$xname)) +
scale_y_continuous("MSE test") +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r}
fw<- as.formula(AveragePrice ~ month + TotalBags + Area_TotalUS + Area_Plains + other_PLU)
fit.fw<-lm(fw, data = avo_train)
```

```{r}
## calculate the yhat price for the avo dataset
yhat_avo_fw <- predict(fit.fw, avo)
df1_fw<-avo %>% 
  select(Date, AveragePrice)
df2_fw<-cbind(df1_fw, yhat_avo_fw)
colnames(df2_fw)
names(df2_fw)[3]<-"AveragePrice_hat"
## Plot the actual average price and the predictive average price
plot1_fw <- df2_fw %>% 
  group_by(Date) %>% 
  summarize(
    MeanAvg=mean(AveragePrice),
    MeanAvg_hat=mean(AveragePrice_hat))%>% 
    ggplot()+
    geom_line(aes(Date, MeanAvg),color = "#356211")+
    geom_line(aes(Date, MeanAvg_hat), color = "#cda989")+
    theme_classic()

plot1_fw + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "#AA471F")

```


* ### Backward Selection ###
```{r}
install.packages(ISLR)
library(ISLR)
library(leaps)
regfit.bwd = regsubsets(f1, data = avo_train, nvmax = 19, method = "backward")
summary(regfit.bwd)
```
```{r}
sub_model<-lm(f1, data = avo_train)
yhat_train_stepwise <- predict(sub_model, avo_train)
MSE_train_stepwise <- mean((avo_train$AveragePrice - yhat_train_stepwise)^2)
MSE_train_stepwise
yhat_test_stepwise <- predict(sub_model, avo_test)
MSE_test_stepwise <- mean((avo_test$AveragePrice - yhat_test_stepwise)^2)
MSE_test_stepwise

```
Based on the coefficients, eliminate "Area_TotalUS", "type_organic".
```{r}
f1_1 <- as.formula(AveragePrice ~ month + type_conventional + TotalVolume + 
                      PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                      + Area_NewEngland + Area_Southeast
                    + Area_Mideast + Area_RockyMountain
                    + Area_FarWest + Area_GreatLakes
                    + Area_Southwest
                    + Area_Plains)
regfit.bwd1 = regsubsets(f1_1, data = avo_train, nvmax = 19, method = "backward")
summary(regfit.bwd1)

sub_model1<-lm(f1_1, data = avo_train)
yhat_train_stepwise1 <- predict(sub_model1, avo_train)
MSE_train_stepwise1 <- mean((avo_train$AveragePrice - yhat_train_stepwise1)^2)
MSE_train_stepwise1
yhat_test_stepwise1 <- predict(sub_model1, avo_test)
MSE_test_stepwise1 <- mean((avo_test$AveragePrice - yhat_test_stepwise1)^2)
MSE_test_stepwise1
```
Continue to use backward selection to train the model
Eliminate "LargeBags"
```{r}
f1_2 <- as.formula(AveragePrice ~ month + type_conventional + TotalVolume + 
                     PLU4046 + PLU4770 + PLU4225 + SmallBags + XLargeBags
                     + Area_NewEngland +
                   Area_Mideast + Area_RockyMountain
                   + Area_FarWest + Area_GreatLakes + Area_Southwest + Area_Plains
                   )
regfit.bwd2 = regsubsets(f1_2, data = avo_train, nvmax = 19, method = "backward")
summary(regfit.bwd2)

sub_model2<-lm(f1_2, data = avo_train)
yhat_train_stepwise2 <- predict(sub_model2, avo_train)
MSE_train_stepwise2 <- mean((avo_train$AveragePrice - yhat_train_stepwise2)^2)
MSE_train_stepwise2
yhat_test_stepwise2 <- predict(sub_model2, avo_test)
MSE_test_stepwise2 <- mean((avo_test$AveragePrice - yhat_test_stepwise2)^2)
MSE_test_stepwise2
```
Calculate the yhat price for the avo dataset and merge the date frame
```{r}
yhat_avo_avgprice <- predict(sub_model2, avo)
library(ggplot2)
df1_bwd<-avo %>% 
  select(Date, AveragePrice)
df2_bwd<-cbind(df1_bwd, yhat_avo_avgprice)
colnames(df2_bwd)
names(df2_bwd)[3]<-"AveragePrice_hat"
```

Plot the actual average price and the predictive average price
```{r}
plot1_bwd <- df2_bwd %>% 
  group_by(Date) %>% 
  summarize(
    MeanAvg=mean(AveragePrice),
    MeanAvg_hat=mean(AveragePrice_hat))%>% 
    ggplot()+
    geom_line(aes(Date, MeanAvg),color = "blue")+
    geom_line(aes(Date, MeanAvg_hat), color = "red")+
    theme_classic()

plot1_bwd + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "lightgreen")
```

* ### Ridge Regression ###

run ridge
```{R}
fit_ridge <- cv.glmnet(x1_train, y1_train, alpha = 0, nfolds = 100)
fit_ridge$lambda
```

Predict response
```{R}
yhat_train_ridge <- predict(fit_ridge, x1_train, s = fit_ridge$lambda)
yhat_test_ridge <- predict(fit_ridge, x1_test, s = fit_ridge$lambda)

mse_train_ridge = vector()
mse_test_ridge = vector()
mse_train_ridge <- mean((y1_train - yhat_train_ridge)^2)
mse_test_ridge <-mean((y1_test - yhat_test_ridge)^2)
for (i in 1:length(fit_ridge$lambda)) {
  mse_train_ridge[i] <- mean((y1_train - yhat_train_ridge)[,i]^2)
  mse_test_ridge[i] <- mean((y1_test - yhat_test_ridge)[,i]^2)
}
mse_train_ridge
mse_test_ridge
min(mse_test_ridge)
# 0.1366643
```

```{R}
lambda_min_mse_train<- fit_ridge$lambda[which.min(mse_train_ridge)]
lambda_min_mse_test <-fit_ridge$lambda[which.min(mse_test_ridge)]
lambda_min_mse_train
lambda_min_mse_test


yhat_train_ridge <- predict(fit_ridge, x1_train, s = 0.0261992)
yhat_test_ridge <- predict(fit_ridge, x1_test, s =   0.1160787)
yhat_1<- predict(fit_ridge, x1_avo, s = lambda_min_mse_test)
```

Aggregate data into one dataframe
```{R}
p1<-avo %>%
  select(Date, AveragePrice)
p2<-cbind(p1, yhat_1)
```

## Plot
```{R}
p2 %>%
  group_by(Date)%>%
  summarise(meanpriced = mean(AveragePrice))

names(p2)[3] <- "pre"
p2 %>%
  group_by(Date) %>%
  summarise(meanpriced = mean(AveragePrice),meanpre = mean(pre))%>%
  ggplot()+
  geom_line(mapping = aes(x=Date,
                           y=meanpriced), col = "#356211")+
  geom_line(mapping = aes(x=Date, y= meanpre), col = "#cda989")+
  geom_vline(xintercept=as.numeric(as.Date("2017-03-01")), col = "#AA471F", linetype = "dashed") + 
  labs(title = "Ridge Regression",
       y = "Mean Price")
```
The ridge model is pretty much the same with lasso model, so we will see the difference between two models to see which model gives a more accurate prediction for our dataset after building a lasso model.


* ### Lasso Regression ###

Run lasso
```{r}
lmodel <- glmnet(x1_train, y1_train, alpha = 1, nlambda = 100)
lmodel$lambda
```
Predict response
```{r}
y1_train_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_train)
y1_test_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_test)
length(y1_test_hat)
mse_train = vector()
mse_test = vector()

for (i in 1:length(lmodel$lambda)) {
  mse_train[i] <- mean((y1_train - y1_train_hat)[,i]^2)
  mse_test[i] <- mean((y1_test - y1_test_hat)[,i]^2)
}
mse_train
mse_test
min(mse_train)
min(mse_test)
```
Check the minimun lambda for the train and test dataset
```{r}
lambda_min_mse_train<- lmodel$lambda[which.min(mse_train)]
lambda_min_mse_test <-lmodel$lambda[which.min(mse_test)]
lambda_min_mse_train
lambda_min_mse_test
```
Using Cross-validation fucntion to find the best lambda
```{r}
set.seed(1)
cv.out = cv.glmnet(x1_train, y1_train, alpha = 1)
## plot the lambda
plot(cv.out)
##check the best lambda
bestlam = cv.out$lambda.min
bestlam ## the best lamdba for training dataset is same as lambda_min_mse_train
```
Create a new formula for new predictors(eliminate the uncorrelated predictors)
```{r}
f2 <- as.formula(AveragePrice ~ month + type_conventional + type_organic + 
                   PLU4046 + PLU4770 + PLU4225 + LargeBags + XLargeBags + 
                   Area_NewEngland + Area_Southeast + Area_Mideast + 
                   Area_RockyMountain + Area_GreatLakes + 
                   Area_Southwest + Area_Plains)
x2_train <- model.matrix(f2,avo_train)[,-1]
x2_test <- model.matrix(f2, avo_test)[,-1]
```
Run lasso model again with new predictors
```{r}
lmodel2 <- glmnet(x2_train, y1_train, alpha = 1, nlambda = 100)
```
Predict response with new predictors
```{r}
y2_train_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_train)
y2_test_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_test)
```
Compute MES again with new predictors
The results shows the taining data MSE is increased
When eliminate the predictors which don't have correlation 
But the test data MSE still keep the same
```{r}
mse_train1 = vector()
mse_test1 = vector()

for (i in 1:length(lmodel2$lambda)) {
  mse_train1[i] <- mean((y1_train - y2_train_hat)[,i]^2)
  mse_test1[i] <- mean((y1_test - y2_test_hat)[,i]^2)
}
mse_train1
mse_test1
min(mse_train1)
min(mse_test1)
```
Check again with the new minumue lambda for the train and test dataset
```{r}
lambda_min_mse_train1<- lmodel2$lambda[which.min(mse_train1)]
lambda_min_mse_test1 <-lmodel2$lambda[which.min(mse_test1)]
lambda_min_mse_train1
lambda_min_mse_test1
```
Check coefficients for f1 and f2 model
```{r}
f1coef<-coef(lmodel, s = lambda_min_mse_test)
f2coef<-coef(lmodel2, s = lambda_min_mse_test1)
f1coef 
f2coef
```
Since the second model have the training MSE increased, 
The ideal model is still the first "lmodel"
We decide to use "lmodel" to run the prediction
```{r}
y1_avo_min_lambda_hat <- predict(lmodel, s = lambda_min_mse_test, newx = x1_avo)
class(y1_avo_min_lambda_hat)
```
Aggregate data into one dataframe for the first model prediction
```{r}
df1<-avo %>% 
  select(Date, AveragePrice)
df2<-cbind(df1, y1_avo_min_lambda_hat)
colnames(df2)

names(df2)[3]<-"AveragePrice_hat"
```
Plot the actual average price and the predictive average price
```{r}
head(df2)
class(df2$Date)
plot1 <- df2 %>% 
  group_by(Date) %>% 
  summarize(
    MeanAvg=mean(AveragePrice),
    MeanAvg_hat=mean(AveragePrice_hat)) %>%
  ggplot()+
  geom_line(aes(Date, MeanAvg),color = "blue")+
  geom_line(aes(Date, MeanAvg_hat), color = "red")+
  theme_classic()

plot1 + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "lightgreen")
```
We can see that our ridge model actually did a better job than lasso. 
Ridge is more flexibly than lasso, maybe each variable have something to do in our data set, so ridge works better than lasso. 

* ### Decision Tree ###

```{r}
fit.tree <- rpart(f1,
                  avo_train,
                  control = rpart.control(cp = 0.001))

par(xpd = TRUE)
plot(fit.tree, compress=TRUE)
text(fit.tree, use.n=TRUE)
rpart.plot(fit.tree, type = 1)
```

```{r}
yhat.train.tree <- predict(fit.tree, avo_train)
mse.train.tree <- mean((avo_train$AveragePrice - yhat.train.tree)^2)
mse.train.tree
# 0.03397259
```

```{r}
yhat.test.tree <- predict(fit.tree, avo_test)
mse.test.tree <- mean((avo_test$AveragePrice - yhat.test.tree)^2)
mse.test.tree
# 0.1468156
```

* ### Regression Tree ###

```{r}
tree.avo =tree(f1, avo_train)
summary(tree.avo)
```

```{r}
cv.avo =cv.tree(tree.avo)

prune.avo =prune.tree(tree.avo,best =5)
plot(prune.avo)
text(prune.avo,pretty =0)
```


```{r}
yhat <- predict(tree.avo, newdata = avo_test)
mse_regreTree_test <- mean((yhat - avo_test$AveragePrice)^2)
mse_regreTree_test
# 0.1499934
```


* ### Bagging ###

```{r}
set.seed (1)
bag.avo =randomForest(f1, data = avo_train, 
                           mtry = 19, importance =TRUE)
bag.avo
```

```{r}
yhat.bag = predict (bag.avo, newdata = avo_test)
plot(yhat.bag, avo_test$AveragePrice, main = "Scatter Plot for Bagging",
     xlab = "Prediction price in test set", ylab = "Average price in test set", 
     col = "#bdcc64")
abline (0,1)

mse_bag_test <- mean((yhat.bag - avo_test$AveragePrice)^2)
mse_bag_test
```


* ### Random Forests ###

Building the model first:

```{r}
fit_rf <- randomForest(f1,
                       avo_train,
                       ntree=300,
                       do.trace=F)
varImpPlot(fit_rf)
```
Predicting our y_hats for train and for test data:

```{r}
yhat_rf_train <- predict(fit_rf, avo_train)
yhat_rf_test <- predict(fit_rf, avo_test)
```

Calculating MSE for both train and test:

```{r}
mse_rf_train <- mean((yhat_rf_train - y1_train) ^ 2)
mse_rf_test <- mean((yhat_rf_test - y1_test)^2)

print(mse_rf_train)
print(mse_rf_test)
```

Plotting actual prices and predicted prices on one plot:
Have to prepare data to use it for the plot first by creating a dataframe with Date, AveragePrice and Predicted Price (y_hat) for both train and test:

```{r}
avo %>% 
  select(Date, AveragePrice) -> plot_rf_full

yhat_rf_full <- c(yhat_rf_train,yhat_rf_test )

cbind(plot_rf_full, yhat_rf_full) -> plot_rf_full1  
```



```{r}
plot_rf_full1 %>% 
  group_by(Date) %>% 
  summarise(mean_y = mean(AveragePrice),
            mean_yhat = mean(yhat_rf_full)) %>% 
  ggplot() +
  geom_line(aes(x = Date, y = mean_y), col = "#356211") +
  geom_line(aes(x = Date, y = mean_yhat), col = "#cda989") -> rf_graph
 
rf_graph + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "#AA471F") + 
  labs(title = "Random Forest",
       y = "Mean Price")
```
Conclusion for random forest: As we can see on the graph above, with 300 trees, random forest fit train data pretty well, however it is doing bad with test data. Nevertheless, the graph looks better than other less flexible models we have used.
We could have tried to increase the number of trees to see if it would give better estimations, however, unfortunately, the capacity of RStudios that we have used is not enough to run random forests with more than 300 trees.




* ### Boosting ###

```{r}
boostingcv <- gbm(f1,
                  data = avo_train,
                  distribution = "gaussian",
                  n.trees = 150,
                  interaction.depth = 4,
                  cv.folds = 10,
                  shrinkage = 0.1)
relative.influence(boostingcv)

yhat_btree <- predict(boostingcv, avo_train, n.trees = 150)
mse_btree <- mean((yhat_btree - y1_train) ^ 2)
print(mse_btree)
```

```{r}
yhat_btree_test <- predict(boostingcv, avo_test, n.trees = 150)
mse_btree_test <- mean((yhat_btree_test - y1_test) ^ 2)
print(mse_btree_test)
```

```{r}
avo_train$prediction_btree <- yhat_btree
avo_test$prediction_btree <- yhat_btree_test

avo_plot <- rbind(avo_train, avo_test)
```

```{r}
btree_plot <- avo_plot %>% 
  group_by(Date) %>% 
  summarise(meanAvg = mean(AveragePrice),
            meanAvg_hat = mean(prediction_btree)) %>% 
  ggplot() +
  geom_line(aes(Date, meanAvg), col = "#356211") + 
  geom_line(aes(Date, meanAvg_hat), col = "#cda989") + 
  labs(title = "Boosting",
       y = "Mean Price") +
  theme_clean()

btree_plot + 
  geom_vline(aes(xintercept = as.numeric(Date[113])),
             linetype = "dashed", size = 1, col = "#AA471F")
```


* ### Lasso with a New Variable ###
```{r}
ds1 <- read_csv("avocado3.csv")
glimpse(ds1)
```
The new dataset called `avocado3` is the dataset that we created based on the original avocado dataset. We create two more columns called "NewPrice" and "NewPrice2" which was based on the previous price that we have as "AveragePrice". The NewPrice was come from the AveragePrice of one row above and the "NewPrice2" was from  the AveragePrice of two rows above. We are going to add these two columns as new variables to do a more accurate prediction for our time-series based dataset. 

```{r}
ds1 <- ds1 %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)

ds1 <- ds1 %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)

colName <- names(ds1)
colName[1] <- "ID"
names(ds1) <- colName

ds1$ID <- seq(nrow(ds1))

ds1$month <- month(ds1$Date)
ds1 <- ds1 %>% 
  select(ID, year, month, everything())

ds1New <- dummy_cols(ds1, select_columns = "type") %>% 
  select(ID, year, month, region, type_conventional, type_organic, 
         everything(), -type)

ds1New$other_PLU <- ds1New$`Total Volume` - ds1New$`4046` - ds1New$`4225` - ds1New$`4770`

ds1New <- ds1New %>% 
  select(1:3, Date, everything())

uniqueRegion <- unique(ds1New$region)
uniqueRegion <- as.data.frame(uniqueRegion)
uniqueRegion$Area <- NA
uniqueRegion$Area[1] <- "NewEngland"
uniqueRegion$Area[2] <- "Southeast"
uniqueRegion$Area[3] <- "Mideast"
uniqueRegion$Area[4] <- "RockyMountain"
uniqueRegion$Area[5] <- "NewEngland"
uniqueRegion$Area[6] <- "Mideast"
uniqueRegion$Area[7] <- "FarWest"
uniqueRegion$Area[8] <- "Southeast"
uniqueRegion$Area[9] <- "GreatLakes"
uniqueRegion$Area[10] <- "GreatLakes"
uniqueRegion$Area[11] <- "GreatLakes"
uniqueRegion$Area[12] <- "Southwest"
uniqueRegion$Area[13] <- "RockyMountain"
uniqueRegion$Area[14] <- "GreatLakes"
uniqueRegion$Area[15] <- "GreatLakes"
uniqueRegion$Area[16] <- "GreatLakes"
uniqueRegion$Area[17] <- "Mideast"
uniqueRegion$Area[18] <- "NewEngland"
uniqueRegion$Area[19] <- "Southeast"
uniqueRegion$Area[20] <- "GreatLakes"
uniqueRegion$Area[21] <- "Southeast"
uniqueRegion$Area[22] <- "FarWest"
uniqueRegion$Area[23] <- "FarWest"
uniqueRegion$Area[24] <- "Southeast"
uniqueRegion$Area[25] <- "Southeast"
uniqueRegion$Area[26] <- "Southeast"
uniqueRegion$Area[27] <- "Southeast"
uniqueRegion$Area[28] <- "Southeast"
uniqueRegion$Area[29] <- "Mideast"
uniqueRegion$Area[30] <- "NewEngland"
uniqueRegion$Area[31] <- "NewEngland"
uniqueRegion$Area[32] <- "Southeast"
uniqueRegion$Area[33] <- "Mideast"
uniqueRegion$Area[34] <- "Southwest"
uniqueRegion$Area[35] <- "Mideast"
uniqueRegion$Area[36] <- "Plains"
uniqueRegion$Area[37] <- "FarWest"
uniqueRegion$Area[38] <- "Southeast"
uniqueRegion$Area[39] <- "Southeast"
uniqueRegion$Area[40] <- "Southeast"
uniqueRegion$Area[41] <- "FarWest"
uniqueRegion$Area[42] <- "FarWest"
uniqueRegion$Area[43] <- "FarWest"
uniqueRegion$Area[44] <- "FarWest"
uniqueRegion$Area[45] <- "Southeast"
uniqueRegion$Area[46] <- "Southeast"
uniqueRegion$Area[47] <- "Southeast"
uniqueRegion$Area[48] <- "FarWest"
uniqueRegion$Area[49] <- "Plains"
uniqueRegion$Area[50] <- "Mideast"
uniqueRegion$Area[51] <- "Southeast"
uniqueRegion$Area[52] <- "TotalUS"
uniqueRegion$Area[53] <- "FarWest"
uniqueRegion$Area[54] <- "Southwest"
names(uniqueRegion)[1] <- "region"

avo <- ds1New %>% 
  left_join(uniqueRegion, by = "region") %>% 
  select(1:5, Area, everything())
avo <- dummy_cols(avo, select_columns = "Area")

names(avo)[10] <- "TotalVolume"
names(avo)[14] <- "TotalBags"
names(avo)[15] <- "SmallBags"
names(avo)[16] <- "LargeBags"
names(avo)[17] <- "XLargeBags"
names(avo)[11] <- "PLU4046"
names(avo)[12] <- "PLU4225"
names(avo)[13] <- "PLU4770"

set.seed(1234)
avo_train <- avo %>% filter(as.Date(Date) < "2017-03-01")
avo_train %>%
  filter(year == 2017, month == 2)
avo_test <- avo %>% filter(as.Date(Date) >= "2017-03-01")
avo_test %>%
  filter(year == 2018, month == 3)
```

pre model
```{r}
f2 <- as.formula(AveragePrice ~ month +type_conventional + type_organic + TotalVolume + 
                   PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                   + Area_NewEngland + Area_Southeast
                 + Area_Mideast + Area_RockyMountain
                 + Area_FarWest 
                 + Area_GreatLakes + Area_Southwest
                 + Area_Plains + Area_TotalUS + NewPrice + NewPrice2)

x1_train <- model.matrix(f2,avo_train)[,-1]
y1_train <- avo_train$AveragePrice
x1_test <- model.matrix(f2, avo_test)[,-1]
y1_test <- avo_test$AveragePrice
date_test <- avo_test$Date
x1_avo <- model.matrix(f2, avo)[,-1]
```

Run lasso
```{r}
lmodel <- glmnet(x1_train, y1_train, alpha = 1, nlambda = 100)
lmodel$lambda
```

Predict response
```{r}
y1_train_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_train)
y1_test_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_test)
length(y1_test_hat)
mse_train = vector()
mse_test = vector()

for (i in 1:length(lmodel$lambda)) {
  mse_train[i] <- mean((y1_train - y1_train_hat)[,i]^2)
  mse_test[i] <- mean((y1_test - y1_test_hat)[,i]^2)
}
mse_train
mse_test

min(mse_test)
```

```{r}
lambda_min_mse_train<- lmodel$lambda[which.min(mse_train)]
lambda_min_mse_test <-lmodel$lambda[which.min(mse_test)]
lambda_min_mse_train
lambda_min_mse_test
```


Using Cross-validation fucntion to find the best lambda
```{r}
set.seed(1)
cv.out = cv.glmnet(x1_train, y1_train, alpha = 1)
```

Plot the lambda
```{r}
plot(cv.out)
```

Check the best lambda
```{r}
bestlam = cv.out$lambda.min
bestlam ## the best lamdba for training dataset is same as lambda_min_mse_train
```

Create a new formula for new predictors(eliminate the uncorrelated predictors)
```{r}
f3 <- as.formula(AveragePrice ~ month + type_conventional + type_organic + 
                   PLU4046 + PLU4770 + PLU4225 + LargeBags + XLargeBags + 
                   Area_NewEngland + Area_Southeast + Area_Mideast + 
                   Area_RockyMountain + Area_GreatLakes + 
                   Area_Southwest + Area_Plains)
x2_train <- model.matrix(f3,avo_train)[,-1]
x2_test <- model.matrix(f3, avo_test)[,-1]
```

Run lasso model again with new predictors
```{r}
lmodel2 <- glmnet(x2_train, y1_train, alpha = 1, nlambda = 100)
```

Predict response with new predictors
```{r}
y2_train_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_train)
y2_test_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_test)
```

Compute MES again with new predictors. The results shows the taining data MSE is increased when eliminate the predictors which don't have correlation but the test data MSE still keep the same.
```{r}
mse_train1 = vector()
mse_test1 = vector()

for (i in 1:length(lmodel2$lambda)) {
  mse_train1[i] <- mean((y1_train - y2_train_hat)[,i]^2)
  mse_test1[i] <- mean((y1_test - y2_test_hat)[,i]^2)
}
mse_train1
mse_test1

lambda_min_mse_train1<- lmodel2$lambda[which.min(mse_train1)]
lambda_min_mse_test1 <-lmodel2$lambda[which.min(mse_test1)]
lambda_min_mse_train1
lambda_min_mse_test1
```

Check coefficients for f2 and f3 model
```{r}
f2coef<-coef(lmodel, s = lambda_min_mse_test)
f3coef<-coef(lmodel2, s = lambda_min_mse_test1)
f2coef 
f3coef
```

Since the second model have the training MSE increased, the ideal model is still the first "lmodel", so we decide to use "lmodel" to run the prediction
```{r}
y1_avo_min_lambda_hat <- predict(lmodel, s = lambda_min_mse_test, newx = x1_avo)
class(y1_avo_min_lambda_hat)
```

Aggregate data into one dataframe for the first model prediction
```{r}
df2<-avo %>% 
  select(Date, AveragePrice)
df3<-cbind(df2, y1_avo_min_lambda_hat)
colnames(df3)

names(df3)[3]<-"AveragePrice_hat"
```

Plot the actual average price and the predictive average price
```{r}
head(df3)
class(df3$Date)
 df3 %>% 
  group_by(Date) %>%
  summarise(meanpriced = mean(AveragePrice),meanpre = mean(AveragePrice_hat))%>%
  ggplot()+
  geom_line(mapping = aes(x=Date,
                           y=meanpriced), col = "#356211")+
  geom_line(mapping = aes(x=Date, y= meanpre), col = "#cda989")+
  geom_vline(xintercept=as.numeric(as.Date("2017-03-01")), col = "#AA471F", linetype = "dashed")+
   theme_classic()
```
We can see from this graph, that compare with the regular lasso that we did before, this graph did a much better prediction with our dataset. It not only showed a season based change for avocado price but also have some sort of change within one year period. 
However, Although this model have the lowest Test MSE, we can see that the prediction is not as good as some model that we have before. So it require us to work further on this time series dataset to get more accurate prediction. 



* ### Lasso with a previous day prediction as a variable ###

Because we are working with time series dataset, it is reasonable to assume that the previous day price plays a big part in predicting a next day price. 
Therefore, we have tried to create a code that would predict one day price and use it as a predictor for the next day price. 

Let's make a new dataframe with lagged 1 day price as an additional variable, and change it to zero for test data (as in real life we would not know the previous day price):
```{r}
avo_ts <- avo %>% 
          mutate(lag1_y = lag(AveragePrice))

dim(avo_ts)
avo_ts <- drop_na(avo_ts)
dim(avo_ts)

avo_train_ts <- avo_ts[1:12202, ]
avo_test_ts <- avo_ts[12203:18248, ]
dim(avo_test_ts)

avo_test_ts[2:6046,"lag1_y"] <- 0

```

Running a loop to predict each day using previous day prediction:

```{r}
f_ts <- as.formula(AveragePrice ~ lag1_y + month + type_conventional + type_organic + 
                           PLU4046 + PLU4770 + PLU4225 + LargeBags + XLargeBags + 
                           Area_NewEngland + Area_Southeast + Area_Mideast + 
                           Area_RockyMountain + Area_GreatLakes + 
                           Area_Southwest + Area_Plains)


y_hat_ts_test_f <- avo_test_ts[1,"lag1_y"]
avo_ts1 <- avo_ts
lambda_min_test <- 0.001570601 #the min lambda from lasso model above
check_ts <- vector()
for (i in 1:6045) {
  avo_train_ts <- avo_ts1[1:(12203+i), ]
  avo_test_ts <- avo_ts1[(12204+i):18248, ]
  
  x_ts_train <- model.matrix(f_ts,avo_train_ts)[,-1]
  x_ts_test <- model.matrix(f_ts, avo_test_ts)[,-1]
  
  y_ts_train <- avo_train_ts$AveragePrice
  y_ts_test <- avo_test_ts$AveragePrice
  
  lmodel_ts <- glmnet(x_ts_train, y_ts_train, alpha = 1, nlambda = 100)
  
  y_ts_train_hat <- predict(lmodel_ts, s = lambda_min_test, newx = x_ts_train)
  y_ts_test_hat <- predict(lmodel_ts, s = lambda_min_test, newx = x_ts_test)
  y_hat_ts_test_f <- c(y_hat_ts_test_f, y_ts_test_hat[1])
  (length(y_hat_ts_test_f))
  avo_ts1[(12204+i), "lag1_y"] <- y_ts_test_hat[1]
  
}
```
The loop gives back an error, which we, unfortunately, could not resolve. (Also because this loop takes long time to run).

However, it somehow worked to gather prediction based on previous day prediction as variable. 
We can see it in here:
```{r}
length(y_hat_ts_test_f)
y_hat_ts_test_f <- unlist(y_hat_ts_test_f)
```

It is supposed to have 6046 numbers, but maybe due to the error it lost 2 numbers somewhere in the loop.
We suppose that 2 out of 6046 will not add a significant change, so in order to calculate MSE properly, we have decided to substitute the lost 2 numbers with an average of known 6044:

```{r}
y_hat_ts_test_f <- c(y_hat_ts_test_f, mean(y_hat_ts_test_f), mean(y_hat_ts_test_f))
length(y_hat_ts_test_f)
```

We need to also readjust test data because of the loop changes:

```{r}
avo_test_ts <- avo_ts[12203:18248,]
y_ts_test <- avo_test_ts$AveragePrice
```

We now can calculate MSE:

```{r}
mse_ts_test_f <- mean((y_ts_test - y_hat_ts_test_f)^2)
print(mse_ts_test_f)
```
The MSE shown above is the lowest MSE among all models we have run. This may implicate that for time series dataset it is better to use previous day prediction as a variable for next day prediction.
However, it is quite incovenient and time-consuming to adjust models for variables and predict each row separately. 

Let's see if plot can show the difference.

Need to select and prepare data for the plot first:
```{r}
avo %>% 
  select(Date, AveragePrice) -> plot_ts_full
plot_ts_full <- plot_ts_full[2:nrow(plot_ts_full),]

y_hat_train_ts <-  y_ts_train_hat[1:12202,]
y_hat_ts_full <- c(y_hat_train_ts, y_hat_ts_test_f )

plot_ts <- cbind(plot_ts_full, y_hat_ts_full)
```

Plot:
```{r}
plot_ts %>% 
  group_by(Date) %>% 
  summarise(mean_y = mean(AveragePrice),
            mean_yhat = mean(y_hat_ts_full)) %>% 
  ggplot() +
  geom_line(aes(x = Date, y = mean_y), col = "blue") +
  geom_line(aes(x = Date, y = mean_yhat), col = "red") -> ts_graph
 
ts_graph + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "lightgreen")
```
The graph does not show much of the difference, however we know that it is better because it has lower MSE.



----------

## Result ##

* ### Key to choose the best model ###

* ### The best one ###

* ### Challenges ###

----------

## Conclusions ##

* ### Influential Predictors ###

* ### Reflections ###

----------

### References ###

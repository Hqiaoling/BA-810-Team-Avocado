---
title: "BA810 Team 7: Acovado Price Modeling"
author: Ziqin Ma, Qiaoling Huang, Shihan Li, Chenran Peng, Elmira Ushirova
date: October 16, 2019
output: html_notebook
---

----------

## Introduction ##

* ### Business Setting ###

(who cares and why)

* ### Business Problem ###

--------------

## Dataset Debrief ##

* ### Source ###

* ### Descriptive Analysis ###

----------

## Data preparation ##

Before looking at the dataset, we first load the packages that will be used.
```{r}
options(stringsAsFactors = FALSE)

library(tidyverse)
library(lubridate)
library(ggplot2)
library(ggthemes)
library(fastDummies)
library(randomForest)
library(gbm)

# Set the ggtheme beforehead for all plots
theme_set(theme_clean())

```

Then we load the original kaggle dataset and print a quick view of columns and data types.
```{r}
ds <- read_csv("avocado.csv")  
glimpse(ds)
```
There are 14 columns in this dataset, and we will use `AveragePrice` as our prediction. `X1` gives a id for each observation, and we want to transfer that into row numbers for each row by adding 1. Most of the data types are `<dbl>`, and there are only 2 columns with character vectors, `type` and `region`. We will transfer them to dummy variables to train our models.

In the following code chunk, we took multiple steps to arrange the columns.
```{r}
# Switch the order of rows
ds <- ds %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)

# Change `X1` to `ID`
colName <- names(ds)
colName[1] <- "ID"
names(ds) <- colName

# Assign distinct number id to each observation in `ID` column
ds$ID <- seq(nrow(ds))

# Extract month from `Date` column
ds$month <- month(ds$Date)
ds <- ds %>% 
  select(ID, year, month, everything())

```


There are only 2 types of avocados recorded, conventional and organic. We decided to separate `type` column into `type_conventional` and `type_organic`.
```{r}
dsNew <- dummy_cols(ds, select_columns = "type") %>% 
  select(ID, year, month, region, type_conventional, type_organic, 
         everything(), -type)

```


The Product Look Up (PLU) codes are used by grocery retailers to make differnet product inventories. This dataset includes 3 PLU avocados, and we added a new column `other_PLU` to see the volume sold in products without PLU. The number is calculated by substracting PLU volumes from total volumes.
```{r}
dsNew$other_PLU <- dsNew$`Total Volume` - dsNew$`4046` - dsNew$`4225` - dsNew$`4770`

# Reorder the columes
dsNew <- dsNew %>% 
  select(1:3, Date, everything())
```
we initially included `other_PLU` as a predictor to our models, but then we found that the 


Categorize `region` into US Areas
```{r}
uniqueRegion <- unique(dsNew$region)
uniqueRegion <- as.data.frame(uniqueRegion)
uniqueRegion$Area <- NA
uniqueRegion$Area[1] <- "NewEngland"
uniqueRegion$Area[2] <- "Southeast"
uniqueRegion$Area[3] <- "Mideast"
uniqueRegion$Area[4] <- "RockyMountain"
uniqueRegion$Area[5] <- "NewEngland"
uniqueRegion$Area[6] <- "Mideast"
uniqueRegion$Area[7] <- "FarWest"
uniqueRegion$Area[8] <- "Southeast"
uniqueRegion$Area[9] <- "GreatLakes"
uniqueRegion$Area[10] <- "GrateLakes"
uniqueRegion$Area[11] <- "GrateLakes"
uniqueRegion$Area[12] <- "Southwest"
uniqueRegion$Area[13] <- "RockyMountain"
uniqueRegion$Area[14] <- "GrateLakes"
uniqueRegion$Area[15] <- "GrateLakes"
uniqueRegion$Area[16] <- "GrateLakes"
uniqueRegion$Area[17] <- "Mideast"
uniqueRegion$Area[18] <- "NewEngland"
uniqueRegion$Area[19] <- "Southeast"
uniqueRegion$Area[20] <- "GrateLakes"
uniqueRegion$Area[21] <- "Southeast"
uniqueRegion$Area[22] <- "FarWest"
uniqueRegion$Area[23] <- "FarWest"
uniqueRegion$Area[24] <- "Southeast"
uniqueRegion$Area[25] <- "Southeast"
uniqueRegion$Area[26] <- "Southeast"
uniqueRegion$Area[27] <- "Southeast"
uniqueRegion$Area[28] <- "Southeast"
uniqueRegion$Area[29] <- "Mideast"
uniqueRegion$Area[30] <- "NewEngland"
uniqueRegion$Area[31] <- "NewEngland"
uniqueRegion$Area[32] <- "Southeast"
uniqueRegion$Area[33] <- "Mideast"
uniqueRegion$Area[34] <- "Southwest"
uniqueRegion$Area[35] <- "Mideast"
uniqueRegion$Area[36] <- "Plains"
uniqueRegion$Area[37] <- "FarWest"
uniqueRegion$Area[38] <- "Southeast"
uniqueRegion$Area[39] <- "Southeast"
uniqueRegion$Area[40] <- "Southeast"
uniqueRegion$Area[41] <- "FarWest"
uniqueRegion$Area[42] <- "FarWest"
uniqueRegion$Area[43] <- "FarWest"
uniqueRegion$Area[44] <- "FarWest"
uniqueRegion$Area[45] <- "Southeast"
uniqueRegion$Area[46] <- "Southeast"
uniqueRegion$Area[47] <- "Southeast"
uniqueRegion$Area[48] <- "FarWest"
uniqueRegion$Area[49] <- "Plains"
uniqueRegion$Area[50] <- "Mideast"
uniqueRegion$Area[51] <- "Southeast"
uniqueRegion$Area[52] <- "TotalUS"
uniqueRegion$Area[53] <- "FarWest"
uniqueRegion$Area[54] <- "Southwest"
names(uniqueRegion)[1] <- "region"

avo <- dsNew %>% 
  left_join(uniqueRegion, by = "region") %>% 
  select(1:5, Area, everything())


avo <- dummy_cols(avo, select_columns = "Area")

```


Rename Column Names
```{r}
colnames(avo)

names(avo)[10] <- "TotalVolume"
names(avo)[14] <- "TotalBags"
names(avo)[15] <- "SmallBags"
names(avo)[16] <- "LargeBags"
names(avo)[17] <- "XLargeBags"
names(avo)[11] <- "PLU4046"
names(avo)[12] <- "PLU4225"
names(avo)[13] <- "PLU4770"
```

----------

## Create Train and Test Datasets ##

```{r}
set.seed(1234)
avo_train <- avo %>% filter(as.Date(Date) < "2017-03-01")
avo_train %>%
  filter(year == 2017, month == 2)
avo_test <- avo %>% filter(as.Date(Date) >= "2017-03-01")
avo_test %>%
  filter(year == 2018, month == 3)


```


pre model
```{r}
f1 <- as.formula(AveragePrice ~ month+type_conventional + type_organic + TotalVolume + 
                   PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                   + Area_NewEngland + Area_Southeast
                 + Area_Mideast + Area_RockyMountain
                 + Area_FarWest + Area_GreatLakes
                 + Area_Southwest + Area_Plains + Area_TotalUS)

x1_train <- model.matrix(f1,avo_train)[,-1]
y1_train <- avo_train$AveragePrice
x1_test <- model.matrix(f1,avo_test)[,-1]
y1_test <- avo_test$AveragePrice

```

----------

## Modeling ##

* ### Linear Regression ###

* ### Forward Selection ###

* ### Backward Selection ###

* ### Ridge Regression ###


##run ridge
fit_ridge <- cv.glmnet(x1_train, y_train, alpha = 0, nfolds = 100)
fit_ridge$lambda
##Predict response
yhat_train_ridge <- predict(fit_ridge, x1_train, s = fit_ridge$lambda)
View(yhat_train_ridge)
yhat_test_ridge <- predict(fit_ridge, x1_test, s = fit_ridge$lambda)
View(yhat_test_ridge)

mse_train_ridge = vector()
mse_test_ridge = vector()
mse_train_ridge <- mean((y_train - yhat_train_ridge)^2)
mse_test_ridge <-mean((y_test-yhat_test_ridge)^2)
for (i in 1:length(fit_ridge$lambda)) {
  mse_train_ridge[i] <- mean((y_train - yhat_train_ridge)[,i]^2)
  mse_test_ridge[i] <- mean((y_test - yhat_test_ridge)[,i]^2)
}
mse_train_ridge
mse_test_ridge
min(mse_test_ridge)

lambda_min_mse_train<- fit_ridge$lambda[which.min(mse_train_ridge)]
lambda_min_mse_test <-fit_ridge$lambda[which.min(mse_test_ridge)]
lambda_min_mse_train
lambda_min_mse_test


yhat_train_ridge <- predict(fit_ridge, x1_train, s = 0.0261992)
View(yhat_train_ridge)
yhat_test_ridge <- predict(fit_ridge, x1_test, s =   0.1160787)
View(yhat_test_ridge)
yhat_1<- predict(fit_ridge, x1_avo, s = lambda_min_mse_test)
## Aggregate data into one dataframe
p1<-avo %>%
  select(Date, AveragePrice)
p2<-cbind(p1, yhat_1)
View(p2)

## Plot
p2 %>%
  group_by(Date)%>%
  summarise(meanpriced = mean(AveragePrice))
View(p2)
names(p2)[3] <- "pre"
p2 %>%
  group_by(Date) %>%
  summarise(meanpriced = mean(AveragePrice),meanpre = mean(pre))%>%
  ggplot()+
  geom_line(mapping = aes(x=Date,
                           y=meanpriced), col = "#356211")+
  geom_line(mapping = aes(x=Date, y= meanpre), col = "#cda989")+
  geom_vline(xintercept=as.numeric(as.Date("2017-03-01")), col = "#AA471F", linetype = "dashed")


* ### Lasso Regression ###

* ### Decision Tree ###

* ### Regression Tree ###

* ### Bagging ###

* ### Random Forests ###

Building the model first:

```{r}
fit_rf <- randomForest(f1,
                       avo_train,
                       ntree=300,
                       do.trace=F)
varImpPlot(fit_rf)
```
Predicting our y_hats for train and for test data:

```{r}
yhat_rf_train <- predict(fit_rf, avo_train)
yhat_rf_test <- predict(fit_rf, avo_test)
```

Calculating MSE for both train and test:

```{r}
mse_rf_train <- mean((yhat_rf_train - y1_train) ^ 2)
mse_rf_test <- mean((yhat_rf_test - y1_test)^2)

print(mse_rf_train)
print(mse_rf_test)
```

Plotting actual prices and predicted prices on one plot:
Have to prepare data to use it for the plot first by creating a dataframe with Date, AveragePrice and Predicted Price (y_hat) for both train and test:

```{r}
avo %>% 
  select(Date, AveragePrice) -> plot_rf_full

yhat_rf_full <- c(yhat_rf_train,yhat_rf_test )

cbind(plot_rf_full, yhat_rf_full) -> plot_rf_full1  
```



```{r}
plot_rf_full1 %>% 
  group_by(Date) %>% 
  summarise(mean_y = mean(AveragePrice),
            mean_yhat = mean(yhat_rf_full)) %>% 
  ggplot() +
  geom_line(aes(x = Date, y = mean_y), col = "blue") +
  geom_line(aes(x = Date, y = mean_yhat), col = "red") -> rf_graph
 
rf_graph + geom_vline(aes(xintercept = as.numeric(Date[113])), 
                   linetype = "dashed", size = 1,
                   color = "lightgreen")
```
Conclusion for random forest: As we can see on the graph above, with 300 trees, random forest fit train data pretty well, however it is doing bad with test data. Nevertheless, the graph looks better than other less flexible models we have used.
We could have tried to increase the number of trees to see if it would give better estimations, however, unfortunately, the capacity of RStudios that we have used is not enough to run random forests with more than 300 trees.




* ### Boosting ###


<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
Run lasso
=======
=======
=======
>>>>>>> fd8680707e5914cd91109958d2aeb6d3a318e51e
* ### Lasso with a New Variable ###
```{r}
options(stringsAsFactors = FALSE)
install.packages("fastDummies")
install.packages("glmnet")
library(readr)
library(tidyverse)
library(lubridate)
library(fastDummies)
library(glmnet)
<<<<<<< HEAD
>>>>>>> fd8680707e5914cd91109958d2aeb6d3a318e51e
=======
>>>>>>> fd8680707e5914cd91109958d2aeb6d3a318e51e
ds <- read_csv("/cloud/project/avocado3.csv")

View(ds)
```
The new dataset called avocado3 is the dataset that we created based on the original avocado dataset. We create two more columns called "NewPrice" and "NewPrice2" which was based on the previous price that we have as "AveragePrice". The NewPrice was come from the AveragePrice of one row above and the "NewPrice2" was from  the AveragePrice of two rows above. We are going to add these two columns as new variables to do a more accurate prediction for our time-series based dataset. 
<<<<<<< HEAD

# Switch the order of rows
```{r}
ds <- ds %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)
```

=======

# Switch the order of rows
```{r}
ds <- ds %>% 
  group_by(type, region) %>% 
  select(X1, year, Date, type, region, everything()) %>% 
  arrange(Date)
```

>>>>>>> fd8680707e5914cd91109958d2aeb6d3a318e51e
# Change `X1` to `ID`
```{r}
colName <- names(ds)
colName[1] <- "ID"
names(ds) <- colName
```
# Assign distinct number id to each observation in `ID` column
```{r}
ds$ID <- seq(nrow(ds))
glimpse(ds)
```
# Add a `month` column
```{r}
ds$month <- month(ds$Date)
ds <- ds %>% 
  select(ID, year, month, everything())
```
# Subset `type` column into `conventional` and `organic`
```{r}
dsNew <- dummy_cols(ds, select_columns = "type") %>% 
  select(ID, year, month, region, type_conventional, type_organic, 
         everything(), -type)
```
# Other types
```{r}
dsNew$other_PLU <- dsNew$`Total Volume` - dsNew$`4046` - dsNew$`4225` - dsNew$`4770`

dsNew <- dsNew %>% 
  select(1:3, Date, everything())
```
# Categorize `region` into US Areas
```{r}
uniqueRegion <- unique(dsNew$region)
uniqueRegion <- as.data.frame(uniqueRegion)
uniqueRegion$Area <- NA
uniqueRegion$Area[1] <- "NewEngland"
uniqueRegion$Area[2] <- "Southeast"
uniqueRegion$Area[3] <- "Mideast"
uniqueRegion$Area[4] <- "RockyMountain"
uniqueRegion$Area[5] <- "NewEngland"
uniqueRegion$Area[6] <- "Mideast"
uniqueRegion$Area[7] <- "FarWest"
uniqueRegion$Area[8] <- "Southeast"
uniqueRegion$Area[9] <- "GreatLakes"
uniqueRegion$Area[10] <- "GrateLakes"
uniqueRegion$Area[11] <- "GrateLakes"
uniqueRegion$Area[12] <- "Southwest"
uniqueRegion$Area[13] <- "RockyMountain"
uniqueRegion$Area[14] <- "GrateLakes"
uniqueRegion$Area[15] <- "GrateLakes"
uniqueRegion$Area[16] <- "GrateLakes"
uniqueRegion$Area[17] <- "Mideast"
uniqueRegion$Area[18] <- "NewEngland"
uniqueRegion$Area[19] <- "Southeast"
uniqueRegion$Area[20] <- "GrateLakes"
uniqueRegion$Area[21] <- "Southeast"
uniqueRegion$Area[22] <- "FarWest"
uniqueRegion$Area[23] <- "FarWest"
uniqueRegion$Area[24] <- "Southeast"
uniqueRegion$Area[25] <- "Southeast"
uniqueRegion$Area[26] <- "Southeast"
uniqueRegion$Area[27] <- "Southeast"
uniqueRegion$Area[28] <- "Southeast"
uniqueRegion$Area[29] <- "Mideast"
uniqueRegion$Area[30] <- "NewEngland"
uniqueRegion$Area[31] <- "NewEngland"
uniqueRegion$Area[32] <- "Southeast"
uniqueRegion$Area[33] <- "Mideast"
uniqueRegion$Area[34] <- "Southwest"
uniqueRegion$Area[35] <- "Mideast"
uniqueRegion$Area[36] <- "Plains"
uniqueRegion$Area[37] <- "FarWest"
uniqueRegion$Area[38] <- "Southeast"
uniqueRegion$Area[39] <- "Southeast"
uniqueRegion$Area[40] <- "Southeast"
uniqueRegion$Area[41] <- "FarWest"
uniqueRegion$Area[42] <- "FarWest"
uniqueRegion$Area[43] <- "FarWest"
uniqueRegion$Area[44] <- "FarWest"
uniqueRegion$Area[45] <- "Southeast"
uniqueRegion$Area[46] <- "Southeast"
uniqueRegion$Area[47] <- "Southeast"
uniqueRegion$Area[48] <- "FarWest"
uniqueRegion$Area[49] <- "Plains"
uniqueRegion$Area[50] <- "Mideast"
uniqueRegion$Area[51] <- "Southeast"
uniqueRegion$Area[52] <- "TotalUS"
uniqueRegion$Area[53] <- "FarWest"
uniqueRegion$Area[54] <- "Southwest"
names(uniqueRegion)[1] <- "region"

avo <- dsNew %>% 
  left_join(uniqueRegion, by = "region") %>% 
  select(1:5, Area, everything())
View(avo)
avo <- dummy_cols(avo, select_columns = "Area")
View(avo)
```
##### Formatting Done #####


### Rename Column Names ##3
```{r}
avoFormat <- avo
colnames(avoFormat)

names(avo)[10] <- "TotalVolume"
names(avo)[14] <- "TotalBags"
names(avo)[15] <- "SmallBags"
names(avo)[16] <- "LargeBags"
names(avo)[17] <- "XLargeBags"
names(avo)[11] <- "PLU4046"
names(avo)[12] <- "PLU4225"
names(avo)[13] <- "PLU4770"

colnames(avo)
```
### Split the dataset into train and test sets ###
```{r}
set.seed(1234)
avo_train <- avo %>% filter(as.Date(Date) < "2017-03-01")
avo_train %>%
  filter(year == 2017, month == 2)
avo_test <- avo %>% filter(as.Date(Date) >= "2017-03-01")
avo_test %>%
  filter(year == 2018, month == 3)
```

###pre model
```{r}
f2 <- as.formula(AveragePrice ~ month +type_conventional + type_organic + TotalVolume + 
                   PLU4046 + PLU4770 + PLU4225 + SmallBags + LargeBags + XLargeBags + 
                   + Area_NewEngland + Area_Southeast
                 + Area_Mideast + Area_RockyMountain
                 + Area_FarWest 
                 + Area_GrateLakes + Area_Southwest
                 + Area_Plains + Area_TotalUS + NewPrice + NewPrice2)

x1_train <- model.matrix(f2,avo_train)[,-1]
y1_train <- avo_train$AveragePrice
x1_test <- model.matrix(f2, avo_test)[,-1]
y1_test <- avo_test$AveragePrice
date_test <- avo_test$Date
x1_avo <- model.matrix(f2, avo)[,-1]
```
## Run lasso
>>>>>>> e5923f48d4f3b95667042d5f8091a2902c53beea
```{r}
lmodel <- glmnet(x1_train, y1_train, alpha = 1, nlambda = 100)
lmodel$lambda
```

Predict response
```{r}
y1_train_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_train)
y1_test_hat <- predict(lmodel, s = lmodel$lambda, newx = x1_test)
length(y1_test_hat)
mse_train = vector()
mse_test = vector()

for (i in 1:length(lmodel$lambda)) {
  mse_train[i] <- mean((y1_train - y1_train_hat)[,i]^2)
  mse_test[i] <- mean((y1_test - y1_test_hat)[,i]^2)
}
mse_train
mse_test

min(mse_test)

lambda_min_mse_train<- lmodel$lambda[which.min(mse_train)]
lambda_min_mse_test <-lmodel$lambda[which.min(mse_test)]
lambda_min_mse_train
lambda_min_mse_test
```

Using Cross-validation fucntion to find the best lambda
```{r}
set.seed(1)
cv.out = cv.glmnet(x1_train, y1_train, alpha = 1)
```

Plot the lambda
```{r}
plot(cv.out)
```

Check the best lambda
```{r}
bestlam = cv.out$lambda.min
bestlam ## the best lamdba for training dataset is same as lambda_min_mse_train
```

Create a new formula for new predictors(eliminate the uncorrelated predictors)
```{r}
f3 <- as.formula(AveragePrice ~ month + type_conventional + type_organic + 
                   PLU4046 + PLU4770 + PLU4225 + LargeBags + XLargeBags + 
                   Area_NewEngland + Area_Southeast + Area_Mideast + 
                   Area_RockyMountain + Area_GreatLakes + 
                   Area_Southwest + Area_Plains)
x2_train <- model.matrix(f3,avo_train)[,-1]
x2_test <- model.matrix(f3, avo_test)[,-1]
```

Run lasso model again with new predictors
```{r}
lmodel2 <- glmnet(x2_train, y1_train, alpha = 1, nlambda = 100)
```

Predict response with new predictors
```{r}
y2_train_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_train)
y2_test_hat <- predict(lmodel2, s = lmodel2$lambda, newx = x2_test)
```

Compute MES again with new predictors. The results shows the taining data MSE is increased when eliminate the predictors which don't have correlation but the test data MSE still keep the same.
```{r}
mse_train1 = vector()
mse_test1 = vector()

for (i in 1:length(lmodel2$lambda)) {
  mse_train1[i] <- mean((y1_train - y2_train_hat)[,i]^2)
  mse_test1[i] <- mean((y1_test - y2_test_hat)[,i]^2)
}
mse_train1
mse_test1

lambda_min_mse_train1<- lmodel2$lambda[which.min(mse_train1)]
lambda_min_mse_test1 <-lmodel2$lambda[which.min(mse_test1)]
lambda_min_mse_train1
lambda_min_mse_test1
```

Check coefficients for f2 and f3 model
```{r}
f2coef<-coef(lmodel, s = lambda_min_mse_test)
f3coef<-coef(lmodel2, s = lambda_min_mse_test1)
f2coef 
f3coef
```

## Since the second model have the training MSE increased, 
## The ideal model is still the first "lmodel"
## we decide to use "lmodel" to run the prediction
```{r}
y1_avo_min_lambda_hat <- predict(lmodel, s = lambda_min_mse_test, newx = x1_avo)
class(y1_avo_min_lambda_hat)
```
## Aggregate data into one dataframe for the first model prediction
```{r}
df2<-avo %>% 
  select(Date, AveragePrice)
df3<-cbind(df2, y1_avo_min_lambda_hat)
colnames(df3)

names(df3)[3]<-"AveragePrice_hat"
```
## Plot the actual average price and the predictive average price
```{r}
head(df3)
class(df3$Date)
 df3 %>% 
  group_by(Date) %>%
  summarise(meanpriced = mean(AveragePrice),meanpre = mean(AveragePrice_hat))%>%
  ggplot()+
  geom_line(mapping = aes(x=Date,
                           y=meanpriced), col = "#356211")+
  geom_line(mapping = aes(x=Date, y= meanpre), col = "#cda989")+
  geom_vline(xintercept=as.numeric(as.Date("2017-03-01")), col = "#AA471F", linetype = "dashed")+
   theme_classic()
```
We can see from this graph, that compare with the regular lasso that we did before, this graph did a much better prediction with our dataset. It not only showed a season based change for avocado price but also have some sort of change within one year period. 
However, Although this model have the lowest Test MSE, we can see that the prediction is not as good as some model that we have before. So it require us to work further on this time series dataset to get more accurate prediction. 

----------

## Result ##

* ### Which is best ###

* ### Challenges ###

----------

## Conclusions ##

* ### Influential Predictors ###

* ### Reflections ###

----------

### References ###
